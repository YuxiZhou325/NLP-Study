<h1>Read List</h1>
> This is a consolidated area for me to store some useful links I use for studying Machine Learning and Depp Learning <br/>
> Last Update: 15th Nov 2023
<h2>Transformers</h2>

- [深度学习中Dropout层作用](https://blog.csdn.net/PETERPARKERRR/article/details/121888093?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522169974314716800182738775%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=169974314716800182738775&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-1-121888093-null-null.142^v96^pc_search_result_base8&utm_term=dropout%E5%B1%82%E7%9A%84%E4%BD%9C%E7%94%A8&spm=1018.2226.3001.4187)

- [伯努利分布、二项分布、多项分布、Beta分布、Dirichlet分布](https://blog.csdn.net/kingzone_2008/article/details/80584743?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522169974323916800184190392%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=169974323916800184190392&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-1-80584743-null-null.142^v96^pc_search_result_base8&utm_term=%E4%BC%AF%E5%8A%AA%E5%88%A9%E5%88%86%E5%B8%83&spm=1018.2226.3001.4187)

- [似然函数](https://blog.csdn.net/yzy_1996/article/details/89139203?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522169974346616800227476744%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=169974346616800227476744&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-1-89139203-null-null.142^v96^pc_search_result_base8&utm_term=%E4%BC%BC%E7%84%B6%E5%87%BD%E6%95%B0&spm=1018.2226.3001.4187)

- [熬了一晚上，我从零实现了Transformer模型，把代码讲给你听](https://blog.csdn.net/qq_33431368/article/details/121433588?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522169974156216800192295290%2522%252C%2522scm%2522%253A%252220140713.130102334.pc%255Fall.%2522%257D&request_id=169974156216800192295290&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~first_rank_ecpm_v1~rank_v31_ecpm-2-121433588-null-null.142^v96^pc_search_result_base8&utm_term=mutihead%20forward&spm=1018.2226.3001.4187)

- [自注意力(Self-Attention)与Multi-Head Attention机制详解](https://blog.csdn.net/weixin_60737527/article/details/127141542?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522169982075416800182718622%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=169982075416800182718622&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_click~default-1-127141542-null-null.142^v96^pc_search_result_base8&utm_term=multi-head%20self%20attention&spm=1018.2226.3001.4187)

- 
