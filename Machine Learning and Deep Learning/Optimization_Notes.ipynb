{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<h1>Optimization Algorithms in Deep Learning</h1>\n",
    "\n",
    "> #### Author: Zhou Yuxi ####\n",
    "> #### Start Date: 2021-12-17 20:23:24 ####\n",
    "\n",
    "<h2>References:</h2>\n",
    "\n",
    "- [深度学习总结（一）各种优化算法](https://blog.csdn.net/qq_23269761/article/details/80901411)\n",
    "- [码农王小呆](https://blog.csdn.net/manong_wxd/article/details/78735439)\n",
    "- [深度学习最全优化方法总结](https://blog.csdn.net/u012759136/article/details/52302426)\n",
    "- [超级详细每个算法的讲解，可参考](https://blog.csdn.net/tsyccnh/article/details/76673073)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h2>Introduction to Optimization Algorithms</h2>\n",
    "\n",
    "<h3> 1. Batch Gradient Descent (BGD) </h3>\n",
    "\n",
    "$\\theta=\\theta-\\eta\\cdot\\nabla\\theta J(\\theta)$\n",
    "\n",
    "For each iteration step, all data in the training set are used, and the gradient calculated each time is averaged.\n",
    "\n",
    "Here $\\eta \\ $ is the **learning rate**\n",
    "\n",
    "<h3> 2. Stochastic Gradient Descent (SGD) </h3>\n",
    "\n",
    "$\\theta=\\theta-\\eta\\cdot\\nabla\\theta J(\\theta; x(i); y(i))$\n",
    "\n",
    "Iteratively update once through each sample, at the cost of losing a small part of the accuracy and increasing the number of iterations,\n",
    "in exchange for the improvement of the overall optimization efficiency.\n",
    "The increased number of iterations is much smaller than the number of samples.\n",
    "\n",
    "> <h4>Disadvantages: </h4>\n",
    "\n",
    "- **Sensitive to the parameters**, need to pay attention to the initialization of the parameters\n",
    "- Easy to trapped into **Local Minima**\n",
    "- Training time gets long when data is large\n",
    "- All the data in the training set need to be used in every iteration\n",
    "\n",
    "<h3> 3. Mini Batch Gradient Descent (MBGD) </h3>\n",
    "\n",
    "$\\theta=\\theta-\\eta\\cdot\\nabla\\theta J(\\theta; x(i:i+n); y(i:i+n))$\n",
    "\n",
    "In order to avoid the problems in SGD and standard gradient descent, this method only performs one update for n training samples in each batch.\n",
    "(Update the average of all gradients each time)\n",
    "\n",
    "<h3> 4. The Concept of Exponentially Weighted Average <h3>\n",
    "\n",
    "$$v_t=\\beta v_t-1 + (1-\\beta)\\theta_t$$\n",
    "\n",
    "Red Line: $\\beta=0.9$\n",
    "Green Line: $\\beta \\rightarrow 1 \\ (0.98)$\n",
    "Yellow Line: $\\beta=0.5$\n",
    "\n",
    "<img src=\"images/opt_01.png\"> <br>\n",
    "\n",
    "<img src=\"images/opt_02.png\"> <br>\n",
    "\n",
    "<img src=\"images/opt_03.png\"> <br>\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Algorithms | Advantages | Disadvantages | Applicable Situation\n",
    "\n",
    "<img src=\"images/opt_table.png\">"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}