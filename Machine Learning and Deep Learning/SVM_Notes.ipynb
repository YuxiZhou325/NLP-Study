{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "[Original Blog](https://blog.csdn.net/qq_31347869/article/details/88071930?spm=1001.2014.3001.5506)\n",
    "\n",
    "Common Architecture of Machine Learning Strategies: <br>\n",
    "**Training Set -> Extract Feature Vector -> Fitting to Algorithms (Classifier: Decision Tree, kNN, ...) -> Outputs**\n",
    "\n",
    "<h2>SVM</h2>\n",
    "\n",
    "Support Vector Machines (SVMs) is a set of **supervised learning** methods used for classification, regression and outliers detection.\n",
    "It is a **Binary Classification** model, it reflects the instances into dots in the space, and aims to draw a line to distinguish these dots in a best way,\n",
    "so that when there comes new dots, the line can classify them.\n",
    "<span style=\"color:darkorange\">SVMs are suitable for mid-little level data samples, non-linear, high dimension problems.</span>\n",
    "\n",
    "The feature vectors of the instances are mapped to some points in the space (take 2d as example).\n",
    "The purpose of SVM is to draw a line to distinguish the two types of points in the best way,\n",
    "so that if there are new points in the future, this line can also make a good classification.\n",
    "\n",
    "![svm_01](images/svm_01.png)\n",
    "\n",
    "\n",
    "Question: How many lines can be drawn to distinguish the sample points?\n",
    "> There are countless lines that can be drawn. The difference is whether the effect is good or not.\n",
    "> Each line can be called a dividing hyperplane.\n",
    "> For example, the green line above is not good, the blue line is OK, and the red line looks better. The best line we hope to find is the \"division hyperplane with the largest interval\".\n",
    "\n",
    "Question: Why do we call it Hyper-plane?\n",
    "> Because the sample features might be very possibly high dimension, therefore, it won't be a single line to distinguish them.\n",
    "\n",
    "Question: What is the criteria of this line?\n",
    "> The SVM will look for the partitioning hyperplane that can distinguish between the two categories and **maximize the margin**.\n",
    "> It is better to divide the hyperplane, the impact on it is the smallest when the sample is locally disturbed,\n",
    "> the classification result is the most robust, and the generalization ability to unseen examples is the strongest.\n",
    "\n",
    "Question: What is Margin\n",
    "> For any hyperplane, the data points on both sides of it have a **minimum distance (vertical distance)** from it,\n",
    "> and the sum of these two minimum distances is the interval.\n",
    "> For example, the band-shaped area formed by the two dotted lines in the figure below is margin,\n",
    "> and the dotted line is determined by the two points closest to the central solid line (that is, determined by the support vector).\n",
    "> But at this time, the margin is relatively small. If we use the second method to draw, the margin will be significantly larger and closer to our goal.\n",
    "\n",
    "![svm_02](images/svm_02.png)\n",
    "\n",
    "Question: Why make the margin as large as possible?\n",
    "> Because it is less bias when the margin is large, which is robust.\n",
    "\n",
    "Question: What is Support Vector?\n",
    "> As can be seen from the figure above, the distance between the points on the dotted line and the dividing hyperplane is the same.\n",
    "> In fact, only these points jointly determine the position of the hyperplane, so they are called \"support vectors\".\n",
    "> \"Support vector machine\" also came from this.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h2>Hard-Margin SVM</h2>\n",
    "\n",
    "![svm_03](images/svm_03.png)\n",
    "\n",
    "Partition Hyperplane can be defined as a linear equation: $w^T X + b = 0$, while:\n",
    "\n",
    "- $w = \\{ w_1, w_2,...w_d \\}$ is a Normal Vector, which determine the direction of the hyperplane, d is the number of the Eigenvalues\n",
    "- $X$ is the training sample\n",
    "- $b$ is the displacement, which determine the distance between hyperplane and original point.\n",
    "\n",
    "We can determine one unique Partition Hyperplane if and only if normal vector $w$ and displacement $X$ is determined.\n",
    "The distance between any 2 points on Partition Hyperplane and its Margin Hyperplanes at both side is calculated as $ \\frac{1}{||w||} $\n",
    "\n",
    "By using some math derivation, $y_i * (w_0 + w_1x_1 + w_2x_2) \\ge 1 , \\forall i$ becomes restricted Convex Optimization problem\n",
    "\n",
    "By using Karush-Kuhn-Tucker (KKT) condition Lagrangian Equation, it can be concluded that MMH can be expressed as the following **\"decision boundary\"**\n",
    "\n",
    "$$d(X^T) = \\sum_{i=1}^{l} y_i \\alpha_i X_i X^T + b_0$$\n",
    "\n",
    "This equation represents the **Partitioned Hyperplane with the Maximum Margin**\n",
    "\n",
    "- $l$ is the number for Support Vector Points, most of the points are not support vector points, only those who are on the Margin Hyperplanes are Support Vector Points\n",
    "  Thus we only calculate the sum of those points\n",
    "- $X_i$ is the Eigenvalues of the Support Vector Points\n",
    "- $y_i$ is the Class Label of $X_i$, such as +1 or -1\n",
    "- $X^T$ is the instance to be tested, want to know which category it should belong to, put it into the equation\n",
    "- $\\alpha_i$ and $b_0$ are single numerical parameters, obtained by the above-mentioned optimal algorithm, $\\alpha_i$ is Lagrange Constant\n",
    "\n",
    "We make classification according to the equation value (positive or negative) by put new test sample $X$ into the equation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h2>SVM Application Examples</h2>\n",
    "\n",
    "If we have already had two support vector points (1,1) and (2,3), weight is set as $w = (a,2a)$, if we put this two support vector points coordinates\n",
    "in to formula $w^T x + b = \\pm 1$, and we will have\n",
    "$$a + 2a + w_0 = -1$$, using point (1,1)\n",
    "$$2a + 6a + w_0 = 1$$, using point (2,3)\n",
    "\n",
    "We can get $a=\\frac{2}{5}, w_0=-\\frac{11}{5}$ by solving the equation.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}