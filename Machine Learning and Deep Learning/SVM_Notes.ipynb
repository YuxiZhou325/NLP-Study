{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "\n",
    "Common Architecture of Machine Learning Strategies: <br>\n",
    "**Training Set -> Extract Feature Vector -> Fitting to Algorithms (Classifier: Decision Tree, kNN, ...) -> Outputs**\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<h2>SVM</h2>\n",
    "\n",
    "Support Vector Machines (SVMs) is a set of **supervised learning** methods used for classification, regression and outliers detection.\n",
    "It is a **Binary Classification** model, it reflects the instances into dots in the space, and aims to draw a line to distinguish these dots in a best way,\n",
    "so that when there comes new dots, the line can classify them.\n",
    "<span style=\"color:orange\">SVMs are suitable for mid-little level data samples, non-linear, high dimension problems.</span>\n",
    "\n",
    "The feature vectors of the instances are mapped to some points in the space (take 2d as example).\n",
    "The purpose of SVM is to draw a line to distinguish the two types of points in the best way,\n",
    "so that if there are new points in the future, this line can also make a good classification.\n",
    "\n",
    "![svm_01](images/svm_01.png)\n",
    "\n",
    "\n",
    "Question: How many lines can be drawn to distinguish the sample points?\n",
    "> There are countless lines that can be drawn. The difference is whether the effect is good or not.\n",
    "> Each line can be called a dividing hyperplane.\n",
    "> For example, the green line above is not good, the blue line is OK, and the red line looks better. The best line we hope to find is the \"division hyperplane with the largest interval\".\n",
    "\n",
    "Question: Why do we call it Hyper-plane?\n",
    "> Because the sample features might be very possibly high dimension, therefore, it won't be a single line to distinguish them.\n",
    "\n",
    "Question: What is the criteria of this line?\n",
    "> The SVM will look for the partitioning hyperplane that can distinguish between the two categories and **maximize the margin**.\n",
    "> It is better to divide the hyperplane, the impact on it is the smallest when the sample is locally disturbed,\n",
    "> the classification result is the most robust, and the generalization ability to unseen examples is the strongest.\n",
    "\n",
    "Question: What is Margin\n",
    "> For any hyperplane, the data points on both sides of it have a **minimum distance (vertical distance)** from it,\n",
    "> and the sum of these two minimum distances is the interval.\n",
    "> For example, the band-shaped area formed by the two dotted lines in the figure below is margin,\n",
    "> and the dotted line is determined by the two points closest to the central solid line (that is, determined by the support vector).\n",
    "> But at this time, the margin is relatively small. If we use the second method to draw, the margin will be significantly larger and closer to our goal.\n",
    "\n",
    "![svm_02](images/svm_02.png)\n",
    "\n",
    "Question: Why make the margin as large as possible?\n",
    "> Because it is less bias when the margin is large, which is robust.\n",
    "\n",
    "Question: What is Support Vector?\n",
    "> As can be seen from the figure above, the distance between the points on the dotted line and the dividing hyperplane is the same.\n",
    "> In fact, only these points jointly determine the position of the hyperplane, so they are called \"support vectors\".\n",
    "> \"Support vector machine\" also came from this.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<h2>Hard-Margin SVM</h2>\n",
    "\n",
    "![svm_03](images/svm_03.png)\n",
    "\n",
    "Partition Hyperplane can be defined as a linear equation: $w^T X + b = 0$, while:\n",
    "\n",
    "- $w = \\{ w_1, w_2,...w_d \\}$ is a Normal Vector, which determine the direction of the hyperplane, d is the number of the Eigenvalues\n",
    "- $X$ is the training sample\n",
    "- $b$ is the displacement, which determine the distance between hyperplane and original point.\n",
    "\n",
    "We can determine one unique Partition Hyperplane if and only if normal vector $w$ and displacement $X$ is determined.\n",
    "The distance between any 2 points on Partition Hyperplane and its Margin Hyperplanes at both side is calculated as $\\frac{1}{||w||}$\n",
    "\n",
    "By using some math derivation, $y_i * (w_0 + w_1x_1 + w_2x_2) \\ge 1 , \\forall i$ becomes restricted Convex Optimization problem\n",
    "\n",
    "By using Karush-Kuhn-Tucker (KKT) condition Lagrangian Equation, it can be concluded that MMH can be expressed as the following **\"decision boundary\"**\n",
    "\n",
    "$$d(X^T) = \\sum_{i=1}^{l} y_i \\alpha_i X_i X^T + b_0$$\n",
    "\n",
    "This equation represents the **Partitioned Hyperplane with the Maximum Margin**\n",
    "\n",
    "- $l$ is the number for Support Vector Points, most of the points are not support vector points, only those who are on the Margin Hyperplanes are Support Vector Points\n",
    "  Thus we only calculate the sum of those points\n",
    "- $X_i$ is the Eigenvalues of the Support Vector Points\n",
    "- $y_i$ is the Class Label of $X_i$, such as +1 or -1\n",
    "- $X^T$ is the instance to be tested, want to know which category it should belong to, put it into the equation\n",
    "- $\\alpha_i$ and $b_0$ are single numerical parameters, obtained by the above-mentioned optimal algorithm, $\\alpha_i$ is Lagrange Constant\n",
    "\n",
    "We make classification according to the equation value (positive or negative) by put new test sample $X$ into the equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>SVM Application Examples</h2>\n",
    "\n",
    "If we have already had two support vector points (1,1) and (2,3), weight is set as $w = (a,2a)$, if we put this two support vector points coordinates\n",
    "in to formula $w^T x + b = \\pm 1$, and we will have\n",
    "\n",
    "$$a + 2a + w_0 = -1, using \\ point (1,1)$$\n",
    "$$2a + 6a + w_0 = 1, using \\ point (2,3)$$\n",
    "\n",
    "by solving the equation we can get: <br>\n",
    "- $a=\\frac{2}{5}, w_0=-\\frac{11}{5}$\n",
    "- $w=(a,2a)=(\\frac{2}{5}, \\frac{4}{5})$\n",
    "- Partition Hyperplane is $x_1 + 2x_2 - 5.5 = 0$\n",
    "- Using point (2,0) can verify the classification effect of this partition hyperplane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Implement SVM by sklearn\n",
    "\n",
    "from sklearn import svm\n",
    "\n",
    "# define some points and labels\n",
    "X = [[2,0], [1,1], [2,3]]\n",
    "y = [0, 0, 1]\n",
    "\n",
    "# define a classifier\n",
    "clf = svm.SVC(kernel='linear')  # .SVC() is the function of SVM, kernel function is set to linear\n",
    "\n",
    "# train the classifier\n",
    "clf.fit(X,y)  # call the fit function to build the model (which is computing the partition hyperplane and store information in the classifier)\n",
    "\n",
    "print(clf)  # print all tha classifier parameters\n",
    "print(clf.support_)  # print support vectors index\n",
    "print(clf.support_vectors_ )  # print support vectors\n",
    "print(clf.n_support_)  # print how many points are support vectors within each class\n",
    "print(clf.predict([[2,0]]))  # print a new prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "print(__doc__)\n",
    "\n",
    "import numpy as np\n",
    "import pylab as pl  # 绘图功能\n",
    "from sklearn import svm\n",
    "\n",
    "# create 40 points\n",
    "np.random.seed(0) # remain the random sample points unchanged\n",
    "\n",
    "# generate the training sample and make sure it is dividable\n",
    "# np._r means connect the matrix in the row direction\n",
    "# random.randn(a,b) means generate a matrix with a rows and b columns, and random numbers obey Normal Distribution\n",
    "# array(20,2) - [2,2] means minus 2 to the two numbers of each row\n",
    "X = np.r_[np.random.randn(20, 2) - [2, 2], np.random.randn(20, 2) + [2, 2]]\n",
    "\n",
    "# 2 class, 20 points within each class, Y is a column-matrix with 40 rows and 1 column\n",
    "Y = [0] * 20 + [1] * 20\n",
    "\n",
    "# build the SVM model\n",
    "clf = svm.SVC(kernel='linear')\n",
    "clf.fit(X, Y)\n",
    "\n",
    "# get the partition hyperplane\n",
    "# original formular of partition hyperplane：w0x0 + w1x1 + b = 0\n",
    "# transfer to point-slop formula, x0 is x, x1 is y, b is w2\n",
    "# point-slope formula: y = -(w0/w1)x - (w2/w1)\n",
    "w = clf.coef_[0]  # w is a 2-dimensional data, coefficient is [w0,w1]\n",
    "a = -w[0] / w[1]  # slope\n",
    "xx = np.linspace(-5, 5)  # generate some random continuous values between -5 and 5\n",
    "# .intercept[0] get bias, which is b, b / w[1] is intercepts\n",
    "yy = a * xx - (clf.intercept_[0]) / w[1]  # get the linear equation by put x in\n",
    "\n",
    "# draw 2 lines who are parallel to the partition hyperplane and passing through the support vector (same slope, different intercept)\n",
    "b = clf.support_vectors_[0] # bring out the first support vector point\n",
    "yy_down = a * xx + (b[1] - a * b[0])\n",
    "b = clf.support_vectors_[-1] # bring out the last support vector point\n",
    "yy_up = a * xx + (b[1] - a * b[0])\n",
    "\n",
    "# print the parameters\n",
    "print(\"w: \", w)\n",
    "print(\"a: \", a)\n",
    "print(\"support_vectors_: \", clf.support_vectors_)\n",
    "print(\"clf.coef_: \", clf.coef_)\n",
    "\n",
    "# in scikit-learin, coef_: the parameter vector that divides the hyperplane in the linear model is saved. The form is (n_classes, n_features). If n_classes> 1, it is a multi-classification problem, and (1, n_features) is a two-class classification problem.\n",
    "\n",
    "# Draw dividing hyperplane, marginal plane and sample points\n",
    "pl.plot(xx, yy, 'k-')\n",
    "pl.plot(xx, yy_down, 'k--')\n",
    "pl.plot(xx, yy_up, 'k--')\n",
    "# draw the support vectors\n",
    "pl.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1], s=80, facecolors='none')\n",
    "pl.scatter(X[:, 0], X[:, 1], c=Y, cmap=pl.cm.Paired)\n",
    "\n",
    "pl.axis('tight')\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "![svm_04](images/svm_04.png)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h2>Kernel Functions</h2>\n",
    "\n",
    "<h3>Motivation for Using Kernel Functions</h3>\n",
    "\n",
    "When converting to optimization problems in linear SVM, the formula calculations to be solved are all in the form of **dot product**, where $\\phi(X)$ is the conversion of the vector points in the training set to high.\n",
    "\n",
    "For the non-linear mapping function of dimension, the algorithm complexity of the inner product is very large, so we use the kernel function to replace the dot product of the nonlinear mapping function.\n",
    "\n",
    "The dot product of the following kernel function and the nonlinear mapping function is equivalent, but the calculation amount of the kernel function K is much less than the inner product.\n",
    "\n",
    "$$K(X_i, X_j) = \\phi(X_i) \\cdot \\phi(X_j)$$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h3>Common Kernel Functions<h3>\n",
    "\n",
    "- **Polynomial Kernel of Degree h** : $K(X_i,X_j)=(X_i,X_j + 1)^h$\n",
    "- **Gaussian Radial Basis Function Kernel** : $K(X_i,X_j)=e^{-||X_i - X_j||^2/2\\sigma^2}$\n",
    "- **Sigmoid Function Kernel** : $K(X_i,X_j)=tanh(kX_i \\cdot X_j - \\delta)$\n",
    "\n",
    "<h4>How to Choose Kernel</h4>\n",
    "1. According to prior knowledge, image classification -> RBF (Gaussian Radial Basis Function), text usually not use RBF <br>\n",
    "2. Try different kernels, depending on the accuracy to choose kernel function."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h3>Examples of Kernel Functions</h3>\n",
    "\n",
    "Assume that we define two vectors: $x=(x_1,x_2,x_3), y=(y_1,y_2,y_3)$\n",
    "\n",
    "And we define function: $f(x)=(x_1x_1,x_1x_2,x_1x_3,x_2x_1,x_2x_2,x_2x_3,x_3x_1,x_3x_2,x_3x_3)$\n",
    "\n",
    "Kernel Function: $K(x,y)=(<x,y>)^2$\n",
    "\n",
    "Assume that: $x=(1,2,3), y=(4,5,6)$\n",
    "\n",
    "<h4>Calculate Dot Product without Kernel Function: </h4>\n",
    "\n",
    "$f(x)=(1,2,3,2,4,6,3,6,9)$\n",
    "\n",
    "$f(y)=(16,20,24,20,25,36,24,30,36)$\n",
    "\n",
    "$<f(x),f(y)>=16+40+72+40+100+180+72+180+324=1024$\n",
    "\n",
    "<h4>Calculate Dot Product with Kernel Function:</h4>\n",
    "\n",
    "$K(x,y)=(4+10+18)^2=32^2=1024$\n",
    "\n",
    "It is much easier to calculate dot product using the Kernel Function, as they have the same result. Note that this is only the case of 9 dimensions.\n",
    "If the dimension is higher, directly calculating the dot product will be very complex.\n",
    "\n",
    "\n",
    "<h4>The Reason for using Kernel Function: </h4>\n",
    "1. Map the dimension of the vectors from low dimension to high dimension <br>\n",
    "2. Reduce the complexity of the calculation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h2>Linearly Separability</h2>\n",
    "\n",
    "It is linear separable when the sample points can be classified by a straight line, otherwise it is linear inseparable.\n",
    "\n",
    "Some Examples of Linear inseparable:\n",
    "> ![svm_05](images/svm_05.jpg)\n",
    "> <img src=\"images/svm_06.jpg\">\n",
    "> <img src=\"images/svm_07.png\">\n",
    "\n",
    "<h3>The Vectors Corresponding to the Dataset CANNOT be Seperated by a HyperPlane</h3>\n",
    "\n",
    "Two steps for solving this situation:\n",
    "- Use a non-linear mapping to transform the vector points in the original dataset into a **higher-dimensional** space\n",
    "(for example, the following figure maps the points in the two-dimensional space to the three-dimensional space)\n",
    "- Find a linear hyperplane in this high-dimensional space to deal with the linearly separable situation\n",
    "\n",
    "<img src=\"images/svm_08.jpg\">\n",
    "\n",
    "Another example: Mapping $y=x$ into $y=x^2$ to separate red and blue points\n",
    "<img src=\"images/svm_09.png\">\n",
    "<img src=\"images/svm_10.png\">\n",
    "\n",
    "Reference: [SVM with polynomial kernel visualization](https://www.youtube.com/watch?v=3liCbRZPrZA)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h3>Non-Linear Mapping Transfer the Original Data into a Higher Dimension Space</h3>\n",
    "\n",
    "Example:\n",
    "\n",
    "Assume there is a 3-dimensional input vector: $X=(x_1,x_2,x_3)$\n",
    "\n",
    "And we transfer it to a 6-dimensional space Z:\n",
    "\n",
    "$\\phi_1(X)=x_1, \\phi_2(X)=x_2, \\phi_3(X)=x_3, \\phi_4(X)=(x_1)^2, \\phi_5(X)=x_1x_2, \\phi_6(X)=x_1x_3$\n",
    "\n",
    "The new Decision Hyperplane: $d(Z)=WZ+b$, in which W and Z are vectors and the hyperplane is linear\n",
    "\n",
    "Solve the W and b, put it back to the original function, and we will have:\n",
    "\n",
    "$d(Z)=w_1x_1 + w_2x_2 + w_3x_3 + w_4(x_1)^2 + w_5x_1x_2 + w_6x_1x_3 + b = w_1z_1 + w_2z_2 + w_3z_3 + w_4z_4 + w_5z_5 + w_6z_6 +b$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h2>SVM Extend to Multi_Classification Problems</h2>\n",
    "\n",
    "For each class, there is a two-class classifier (one-vs-rest) of the current class and other classes\n",
    "Convert the multi-classification problem into n binary classification problems, where n is the number of categories.\n",
    "\n",
    "<h2>SVM Features</h2>\n",
    "\n",
    "- The algorithm complexity of the trained model is determined by the number of support vectors, not by the dimensionality of the data. So SVM is not easy to produce overfitting.\n",
    "- The model trained by SVM is completely dependent on the support vector. Even if all the non-support vector points in the training set are removed and the training process is repeated,\n",
    "the result will still be exactly the same model.\n",
    "- If the number of support vectors obtained by training for an SVM is relatively small, the model trained by SVM is easier to generalize."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "[Original Blog](https://blog.csdn.net/qq_31347869/article/details/88071930?spm=1001.2014.3001.5506)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}